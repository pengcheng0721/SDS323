---
title: "SDS Exercise 3"
subtitle: "by Cheng Peng, Zhiyuan Wei, Erich Schwartz"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
```

# 1. Predictive Model Buidling

## 1.1 Introduction
We analyzed the greenbuildings.csv dataset, which contains information on 7894 commercial properties across the United States. All of 7894 observations carry 21 distinct features, one of which indicates whether the property is designated as a green building. Our goal is to build the best predictive model for rent and quantify the average change in rental income per square foot associated with green certification. 

## 1.2 Data Cleaning
Critical to this analysis is the variable "green_rating," which indicates a building either has LEED or EnergyStar certification. To prepare the data, we first cleaned the data by removing all N/As from the dataset. Also, as we subsumed both types of certifications under the green building column, we deleted the variables "LEED" and "EnergyStar". We also removed the variable CS_propertyID as the buildings' identifiers couldn't be predictive of their rents. The distribution of rents classified by green certification is as follows:
```{r include=FALSE}
library(tidyverse)
library(mosaic)
library(ISLR)
library(leaps)
library(gamlr)
greenbuildings = read.csv("greenbuildings.csv")
greenbuildings=na.omit(greenbuildings)
greenbuildings$LEED<-NULL
greenbuildings$Energystar<-NULL
greenbuildings$CS_PropertyID<-NULL
Filter(var, greenbuildings)
```
```{r}
theme1 = theme(plot.title = element_text(hjust = 0.5, face = "italic", color = 'grey2'), plot.caption = element_text(hjust = 0.5, face = "italic", color = 'grey3'))
p0=ggplot(greenbuildings)
p0 + 
  geom_boxplot(aes(x=factor(green_rating),y=log(Rent))) +
  theme1 +
  labs(title = 'Boxplot for Rents Classified by Green Certification')
greenbuildings = mutate(greenbuildings, Rent = log(Rent))
```

## 1.3 Lasso Regression 
Next, we employed Lasso Regression in our analysis. We first generated a sparse matrix with log(Rent) and all other variables of green buildings. Next, we used the "gamlr" function in an attempt to find the best lambda.

We returned a path plot for Lasso Regression, featuring the change in regression coefficients for each variable as log(lambda) increases. We then used Cross Validation to identify the best-performing lambda.

The plot shows how the mean squared error varies as log(lambda) increases. It shows that the mean square error steadily increases as lambda increases. Based on this, we select the lambda that returns the smallest out of sample error.

The result shows that the best log(lambda) is -5.630747. At that value, 16 variables are kept in this model. Given that 16 variables are still reasonable to interpret, we choose this model as our best model instead of using the "1-standard-error" model, which is less accurate in comparison. The exact coefficients of our best model are as follows:

```{r}
library(glmnet)
set.seed(111)
x=sparse.model.matrix(Rent~.-1, data=greenbuildings)
y=greenbuildings$Rent
lasso_green = gamlr(x, y)
plot(lasso_green, main = 'Path Plot for Lasso Regression')
# Cross Validation best lambda
cv.out = cv.gamlr(x, y, nfold=10, verb=TRUE)
plot(cv.out, bty="n", main = 'Mean Squared Error vs Log(lambda) in Cross Validation')
gb.min = coef(cv.out, select="min")
gb.min
log(cv.out$lambda.min)
```

## 1.4 Conclusions
By using regularization, more specifically, Lasso regression, we find the best model with log(lambda) at -5.630747 and with 16 variables. Renovated (whether the building has undergone substantial renovations), hd_total07 (number of heating degree days), Gas_Costs, and Electricity_Costs are the four variables eliminated from the model. That appears reasonable as the four variables excluded have limited or no impact on rents. Among the remaining variables: class_a, class_b, green_rating, net, amenities, and cluster_rent are the more robust drivers of rental income. The result matches our intuitions as the quality of the buildings (class_a, class_b), green certifications (green_rating), type of rental contracts (net), the convenience of the location (amenities), and rents of surrounding areas are generally considered to impact the rental income strongly. The final regression coefficients also show that when a building is awarded the green certification, its rental income per square foot increases by 1.91%.

# 2. What causes what?
1. Cities with higher crime rates tend to hire more police. This inherent correlation between the crime rates and the number of cops may return a regression result that says more policing causes higher crime rates. 

2. The researchers looked for examples where the number of cops is not correlated with the crime rates. They used the terrorism alert system in Washington, D.C. When D.C. is on terrorism alert, more police officers will be deployed to the streets. Such deployments are unrelated to the crime rates of D.C. So the researchers could examine whether the crime rates decrease due to the increased policing. The researchers discovered that on high alert days, with more cops on the streets, crimes in D.C. decrease by 7.316 cases, and the result is at 5% significance level. This proves that an increased number of police officers to result in lower crime rates.

3. Researchers were worried that the lower crime rates were the result of fewer tourists on high alert days. So they tried to use midday ridership to measure the number of tourists visiting D.C. and see whether increased policing contributes to lower crime rates when the number of tourists is the same. 

4. The model estimates when D.C. is on high alerts, how the crime rates in Police District 1, and all other districts in D.C. change. Police District 1 is where the White House, Congress, and Smithsonian Institution locate, which will have most of the increased police attention when D.C. is on terrorism alert. The result shows that on high alert days, District 1 will experience a 2.621-case reduction in crimes while in other Districts, the reduction in crimes is not statistically significant. This proves that an increased number of police officers do reduce crime rates.

# 3. Clustering and PCA

## 3.1 Introduction
The wine.csv data contains information on 11 chemical properties of 6497 different bottles of vinho verde wine from northern Portugal. All of the 6497 obsevations carry 13 distinct features including 11 chemical properties; Two other variables that are the quality rate of the wine and the color of the wine. Our goal is to run both clustering and PCA algorithms and determine which dimensionality reduction technique is easily capable of predicting the color of the wine. 

we start by creating a new dataset from wine.csv except we remove the "quality" and "color" columns. Our new dataset new_wine contains information on 11 chemical properties.
```{r}
library(ggplot2)
library(LICORS)
library(foreach)
library(mosaic)
wine = read_csv('wine.csv')
new_wine=wine
new_wine$quality<-NULL
new_wine$color<-NULL
```
## 3.2 Color of the wine

### 3.2.1 PCA Approach

Our first approach is to use PCA. We first construct a PCA model for the dataset. From the summaries, we can derive components that incorporate most information about the dataset. More specifically, PC1, PC2, and PC3 obtained from the dimensionality reduction algorithm can give us information about the dataset with only three variables. Next, we examine the direction of the top three principal components, namely how PCx relates to the 11 chemical properties in detail.

```{r}
pca1<-prcomp(new_wine,scale=TRUE)
summary(pca1)
plot(pca1,type="l", main = 'variances for each principal component')
```

```{r}
loadings = pca1$rotation
```

From the summaries and plots above, we know that PC1, PC2, and PC3 collectively contain 64.36% of the information in the dataset. Thus, we take PC1, PC2, and PC3 as variables of interest for further analysis.

```{r}
a=merge(wine,pca1$x[,1:3],by="row.names")
a=rename(a,A=Row.names)
```

```{r}
ggplot(data = a) +
  geom_point(aes(x=PC1,y=PC2, color=color), size=2) +
  labs(title = 'Scatterplot for PC1 & PC2 vs Color') +
  theme1
```

```{r}
ggplot(data = a) +
  geom_point(aes(x=PC1,y=PC3, color=color), size=2) +
  labs(title = 'Scatterplot for PC1 & PC3 vs Color') +
  theme1
```

We plot the graphs showing the relationship between PC1, PC2 & PC3, and the wines' color. The plots above demonstrate that PC1 appears to be a good indicator of the color of the wine. However, to get this information, we use the "supervised" information within the dataset, namely, the variable "color." Without variable "color," we cannot obtain the fact that PC1 relates to the color of wines (higher PC1 indicates white wine), nor can we know the cutoffs for white/red wine in PC1. Though experienced chemists might infer from the PC1 compositions (which is presented below) that PC1 is related to the color of the wine, this complicates the analysis and fails to simplify the issue for data analyst without chemistry backgrounds. Thus, PCA is not the best dimensionality reduction approach to be employed here.

```{r}
loadings[,1] %>% round(2)
```

(In PC1, citric acid, residual sugar, free sulfur dioxide, and total sulfur dioxide have positive projections while sulphates, fixed and volatile acidity have high negative projections; Experienced chemists may be able to infer from the coefficients above that PC1 is strongly correlated with colors of wines.)

### 3.2.2 Clustering Approach

Now, we attempt the clustering approach. We start by scaling the data, and pick k=2 since the wine we are studying has only two colors: red and white. We return the average chemical composition in both clusters. Details are as follows:
```{r}
X=scale(new_wine,center=TRUE,scale=TRUE)
mu=attr(X,"scaled:center")
sigma=attr(X,"scaled:scale")
#color
cluster1=kmeans(X,2,nstart=25)
```

#### Cluster1:
```{r}
(cluster1$center[1,]*sigma+mu) %>% round(2)
```

#### Cluster2:
```{r}
(cluster1$center[2,]*sigma+mu) %>% round(2)
```

From the plot demonstrating how clustering works with volatile acidity and total sulfur dioxide on the x, y-axis, and the confusion matrix, we can conclude that clustering successfully distinguishes the white wine from the red wine. 98.6% of the observations fall in the clusters of their respective colors. Cluster 1 contains the white wines predominately, whereas Cluster 2 includes mostly red wines.

```{r}
Cluster = factor(cluster1$cluster)
```

```{r}
qplot(volatile.acidity,total.sulfur.dioxide,data=wine,color=Cluster,main="Scatterplot for Volatile Acidity & Total Sulfur Dioxide vs Color") + theme1
reg_matrix = table(wine$color,cluster1$cluster)
names(dimnames(reg_matrix)) <- list('color', 'cluster')
reg_matrix
```

In the PCA method, data analysts have to read from the technicalities of PC1 to infer that PC1 correlates with wines' color, which can be a challenge for anyone without chemistry backgrounds. In clustering, however, the dimensionality reduction technique automatically separates the observations into two groups, each one of which has predominantly one color of wines. Plus, unlike in PCA, we don't need to access the "supervised" information to determine the cutoffs for red/white wines in the clustering approach. Thus, we conclude that clustering is a better methodology than the PCA to distinguish different colors of wines with their chemical properties.

## 3.3 Quality of the wine

Next, we use clustering to distinguish between wines of different qualities. We degsinate that there are three types of wine, namely, the wines of high, average, and low qualities.

```{r}
cluster2=kmeans(X,3,nstart=25)
reg_matrix1 = table(wine$quality,cluster2$cluster)
names(dimnames(reg_matrix1)) <- list('quality', 'cluster')
reg_matrix1
Cluster = factor(cluster2$cluster)
ggplot(wine, aes(x=quality, fill = Cluster)) +
  geom_histogram(position = 'dodge', binwidth = 0.4) +
  labs(title = 'Histogram for Quality of Wines across Different Clusters') + theme1
```

From both the confusion matrix and the plot featuring the distribution of each cluster across different qualities of wine, we find that the clustering can't successfully distinguish between wines of different qualities. All clusters fail to show a concentration over certain levels of quality. Instead, their distributions across varying levels of quality are relatively spread out. Thus, we conclude that clustering is NOT capable of sorting the higher from the lower quality wines using their chemical compositions.

## 3.4 Conclusion
We run both PCA and Clustering algorithm on wine.csv, which contains the chemical composition of wines, and we try to find the dimensionality reduction methodology that is most effective at sorting different colors of wines. Via the analysis of 3 principal components returned from the PCA algorithm, we find it hard to sort different colors of wines without sufficient chemistry backgrounds or accessing "supervised" information. Also, though PCA maximizes the overall variance of the data along a small set of directions to concentrate information, it can well pick directions that make it hard to separate classes. Clustering, however, is proven to have distinguished the red wines from the white ones, with an accuracy of 0.986. Based on the success of the cluetring approach, we apply it to sort wines of different qualities. However, the algorithm fails to successfully sort the higher from the lower quality wines, showing that clustering is also not universal when trying to distinguish between different properties.

# 4. Market Segmentation

## 4.1 Introduction
We have analyzed social media data of certain of NutrientH2O’s twitter followers with a view to informing strategies to enhance the company’s relationship with its users.  The data available to us in the social_marketing.csv database contains 7882 entries, each of which describes the tweets of a particular Company follower. Each follower’s tweets were manually assigned to one or more of 36 different categories representing a broad area of interest, like politics and sports. The entries denote the number of tweets by a given follower of NutrientH20 that fell into a given category. The categories of interests are not mutually exclusive. 

Our goal in analyzing this data was to identify patterns in the twitter activity of Nutrient H2O’s followers that might provide insights into how the owner of NutrientH20 can better target its potential followers.

## 4.2 Clustering
Before the analysis, we removed the variable "X" from the dataset, which is a random identification number unrelated to our study. 

As an initial approach to identifying patterns in the data, we used clustering methodology. After scaling the data, we used hierarchical clustering with average linkage to obtain the dendrogram below:

```{r}
social_marketing <- read.csv("social_marketing.csv")
social_marketing$X <- NULL
sm_scaled = scale(social_marketing, center=TRUE, scale=TRUE)
sm_distance_matrix = dist(sm_scaled, method='euclidean')
hier_sm = hclust(sm_distance_matrix, method='average')
```

```{r}
plot(hier_sm, cex=0.1, ylim = c(10,25), main = 'Hierarchical Clusters of NutrientH2O Followers', xlab='')
```

Next, we cut the tree at k = 20 to obtain 20 distinct clusters and tabulate the number of accounts in each cluster.
```{r}
library(mosaic)
library(foreach)
cluster1 = cutree(hier_sm, k=20)
summary(factor(cluster1))
num = which(cluster1 == 1)
```

From the tabulation of 20 clusters above, we find that cluster 1 contains significantly more observations than the remaining clusters. As the clustering algorithm runs based on the characteristics of users’ tweets, we infer that followers that fall into cluster 1 share similar information preferences. With 97.8% (7724 out of 7894) of the users assigned to cluster 1, and thus sharing similar interests, the most reasonable and profit-maximizing marketing strategy is to target followers within cluster 1. Therefore, in the following analysis, we primarily investigate the interests of audiences of cluster 1, our market segment of interests.

## 4.3 PCA 
To investigate how audiences of cluster 1 differ from the rest of the users in their information preferences, we applied principal component analysis to the social_marketing dataset. Thus, we can decide which topics NutrientH2O should emphasize in order to strengthen its appeal to 97.8% of its users. (We solely focused on how to connect with cluster 1 audiences as from an economic perspective, maximizing appeals to the 97.8% returns better economic profits though we might risk alienating the 2.2%.)
```{r}
pc1 = prcomp(social_marketing, scale = TRUE)
loadings = pc1$rotation
scores = pc1$x
summary(pc1)
loadings = pc1$rotation
interests = pc1$x
```

The principal component analysis summary demonstrates that the first four principal components contain 34.08% of the information within the dataset. Next, we use the four components to predict whether an account will fall on cluster 1 using linear regression.

We generated a new variable denoted cluster_1, which consists of the observations that belong to cluster 1. The new variable is used as the dependent variable in the linear regression, with the four principal components being the explanatory variables.

```{r}
social_marketing <- read.csv("social_marketing.csv")
social_marketing$X <- NULL
social_marketing = merge(social_marketing, pc1$x[,1:4], by="row.names")
social_marketing = mutate(social_marketing, cluster_1 = 0)
social_marketing[num, 42] = 1
lm1 = glm(cluster_1~ PC1 + PC2 + PC3 + PC4, data = social_marketing)
summary(lm1)
in_cluster1 = factor(social_marketing$cluster_1)
qplot(scores[,4], scores[,1], color=in_cluster1, xlab='Component 4', ylab='Component 1', main = 'Scatterplot for PC1 & PC4 vs Cluster1') + theme1
```

The regression result shows that PC1 and PC4 are closest to being statistically significant. Whereas a user of higher PC1 is more likely to be in cluster 1, a user of higher PC4 is less likely to be cluster 1. Thus, the variables in PC1 with positive coefficients and PC4 with negative coefficients are the topics more prevalent among people in cluster 1. The details of PC1 and PC4 is as follows:

### PC1
```{r}
loadings[,1] %>% round(2)
```

All variables in PC1 have negative coefficients. In particular, everyday life topics like sports_fandom, religion, beauty, food, parenting, and family have the highest absolute values. Namely, everyday life topics such as sports_fandom, religion, beauty, food, parenting, and family are subjects that have less appeal to cluster 1 followers than to other followers of NutrientH20. Thus, the account owner should avoid posting topics of the above categories since they're less attractive to the largest group of the account's audiences.

### PC4
```{r}
loadings[,4] %>% round(2)
```

The negative coefficients within PC4 generally feature health-related topics such as personal_fitness and more serious topics like politics and news, which are also likely to be health-related. Given that all categories are not mutually exclusive, posts assigned to politics and news could well be posts regarding healthcare initiative or the development of new drugs. Thus, it's reasonable to infer that a negative PC4 alludes to preferences for health-related topics. Namely, the cluster 1 audiences of the twitter account NutrientH20 engage more on the subjects of health_nutrition, personal_fitness, and outdoors than other groups of audiences. Also, topics more prevalent among youths like college_uni, fashion, and beauty appear to be less attractive to cluster 1 users.

## 4.4 Conclusion
By using hierarchical clustering, we identified a large cluster that is the characterization of typical followers of the account NutrientH20 and believed cluster 1 users are the market segments the owner of NutrientH20 should target. Then, we seek to find the distinctions between the preferences of cluster 1 users and other users. We believe that appealing to cluster 1 users would potentially bring more economic profit despite the risk of alienating other users. Running a regression of a dummy variable denoting cluster_1 and four principal components, we find that PC1 and PC4 are more statistically significant, among others. Then, from the breakdown of PC1 and PC4, we infer that cluster 1 users of NutrientH20 appear to be a population that engages more on the health-related subjects of health_nutrition, personal_fitness, and outdoors than other groups of users. Thus, to better appeal to the bulk of its users, the brand should adjust its marketing approach to associate itself with personal fitness, nutrition, and outdoor activities. Meanwhile, from the breakdown of PC1 and PC4, we find that everyday life subjects like parenting and topics more prevalent among youths like college_uni and fashion are less welcomed by cluster 1 followers. Therefore, marketing around topics such as sports_fandom, religion, beauty, food, parenting, family, college_uni, and fashion is not likely to be fruitful, at least not to the majority of the company’s twitter followers. 

```{r}
Cluster = factor(social_marketing$cluster_1)
```

```{r}
ggplot(social_marketing, aes(x=outdoors, fill = Cluster, stat(density))) +
  geom_histogram(position = 'dodge') +
  labs(title = 'Distribution of Outdoors Tweets in Different Clusters', caption = 'Health-related topics like outdoor activities are more welcomed by cluster 1 users') + theme1
```

```{r}
ggplot(social_marketing, aes(x=college_uni, fill = Cluster, stat(density))) +
  geom_histogram(position = 'dodge') +
  labs(title = 'Distribution of College_uni Tweets in Different Clusters', caption = 'Topics prevalent among youths like college life are less appealing to cluster 1 users') + theme1
```

```{r}
ggplot(social_marketing, aes(x=sports_fandom, fill = Cluster, stat(density))) +
  geom_histogram(position = 'dodge') +
  labs(title = 'Distribution of Fandom Sports Tweets in Different Clusters', caption = 'Everyday life topics like fandom sports are less popular among cluster 1 users') + theme1
```

