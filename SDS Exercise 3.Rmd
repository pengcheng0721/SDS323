---
title: "SDS Exercise 3"
subtitle: "by Cheng Peng, Zhiyuan Wei, Erich Schwartz"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
```

# 1. Predictive Model Buidling

## 1.1 Introduction
The greenbuildings.csv data contains information on 7894 commercial properties across the United States. All of 7894 observations carry 21 distinct features, one of which indicates whether the property is designated as a green building. Our goal is to build the best predictive model for rent and quantify the average change in rental income per square foot associated with green certification. To achieve these objectives, we will apply the knowledge of model selection and regularization in our following analysis.

## 1.2 Data Cleaning
In this analysis, we focus on the variable "green_rating," which indicates a building either has LEED or EnergyStar certification. To build a useful predictive model, we first clean the data by removing all N/As from the dataset and delete the "LEED" and "EnergyStar" columns. We also remove the variable CS_propertyID as the buildings' identifiers can't be predictive of their rents. The distribution of rents classified by green certification is as follows:
```{r include=FALSE}
library(tidyverse)
library(mosaic)
library(ISLR)
library(leaps)
library(gamlr)
greenbuildings = read.csv("/Users/pengcheng/Desktop/greenbuildings.csv")
greenbuildings=na.omit(greenbuildings)
greenbuildings$LEED<-NULL
greenbuildings$Energystar<-NULL
greenbuildings$CS_PropertyID<-NULL
Filter(var, greenbuildings)
```
```{r}
theme1 = theme(plot.title = element_text(hjust = 0.5, face = "italic", color = 'grey2'))
p0=ggplot(greenbuildings)
p0 + 
  geom_boxplot(aes(x=factor(green_rating),y=log(Rent))) +
  theme1 +
  labs(title = 'Boxplot for Rents Classified by Green Certification')
greenbuildings = mutate(greenbuildings, Rent = log(Rent))
```

## 1.3 Lasso Regression 
Next, we employ Lasso Regression in our analysis. We first generate a sparse matrix with log(Rent) and all other variables of green buildings. Next, we use "gamlr" function and attempt to find the best lambda.

We return a path plot for Lasso Regression, featuring the change in regression coefficients for each variable as log(lambda) increases. We then use Cross Validation to obtain the best-performing lambda.

We generate the plot showing how the mean squared error varies as log(lambda) increases. It shows that the mean square error strictly increases as lambda increases. Finally, we select the lambda that returns the smallest out of sample error.

The result shows that the best log(lambda) is -5.630747. 16 variables are kept in this model. Given that 16 variables are still reasonable to interpret, we choose this model as our best model instead of using the "1-standard-error" model, which is less accurate in comparison. The exact coefficients of our best model are as follows:

```{r}
library(glmnet)
set.seed(111)
x=sparse.model.matrix(Rent~.-1, data=greenbuildings)
y=greenbuildings$Rent
lasso_green = gamlr(x, y)
plot(lasso_green, main = 'Path Plot for Lasso Regression')
# Cross Validation best lambda
cv.out = cv.gamlr(x, y, nfold=10, verb=TRUE)
plot(cv.out, bty="n", main = 'Mean Squared Error vs Log(lambda) in Cross Validation')
gb.min = coef(cv.out, select="min")
log(cv.out$lambda.min)
```

## 1.4 Conclusions
By using regularization, more specifically, Lasso regression, we find the best model with log(lambda) at -5.630747 and with 16 variables. Renovated (whether the building has undergone substantial renovations), hd_total07 (number of heating degree days), Gas_Costs, and Electricity_Costs are the four variables eliminated from the model. Namely, the four variables excluded are irrelevant with rents. class_a, class_b, green_rating, net, amenities, and cluster_rent are among the more robust drivers of rental income. The final regression coefficients also show that when a building is awarded the green certification, its rental income per square foot increases by 1.91%.


# 2. What causes what?
1. Cities with higher crime rates tend to hire more police. This inherent correlation between the crime rates and the number of cops may return a regression result that says more policing causes higher crime rates. 

2. The researchers looked for examples where the number of cops is not correlated with the crime rates. They used the terrorism alert system in Washington, D.C. When D.C. is on terrorism alert, more police officers will be deployed to the streets. Such deployments are unrelated to the crime rates of D.C. So the researchers could examine whether the crime rates decrease due to the increased policing. The researchers discovered that on high alert days, with more cops on the streets, crimes in D.C. decrease by 7.316 cases, and the result is at 5% significance level. This proves that an increased number of police officers to result in lower crime rates.

3. Researchers were worried that the lower crime rates were the result of fewer tourists on high alert days. So they tried to use midday ridership to measure the number of tourists visiting D.C. and see whether increased policing contributes to lower crime rates when the number of tourists is the same. 

4. The model estimates when D.C. is on high alerts, how the crime rates in Police District 1, and all other districts in D.C. change. Police District 1 is where the White House, Congress, and Smithsonian Institution locate, which will have most of the increased police attention when D.C. is on terrorism alert. The result shows that on high alert days, District 1 will experience a 2.621-case reduction in crimes while in other Districts, the reduction in crimes is not statistically significant. This proves that an increased number of police officers do reduce crime rates.

# 3. Clustering and PCA

## 3.1 Introduction
The wine.csv data contains information on 11 chemical properties of 6497 different bottles of vinho verde wine from northern Portugal. All of the 6497 obsevations carry 13 distinct features including 11 chemical properties; Two other variables that are the quality rate of the wine and the color of the wine. Our goal is to run both clustering and PCA algorithms and determine which dimensionality reduction technique is easily capable of predicting the color of the wine. 

we start by creating a new dataset from wine.csv except we remove the "quality" and "color" columns. Our new dataset new_wine contains information on 11 chemical properties.
```{r}
library(ggplot2)
library(LICORS)
library(foreach)
library(mosaic)
wine = read_csv('/Users/pengcheng/Desktop/wine.csv')
new_wine=wine
new_wine$quality<-NULL
new_wine$color<-NULL
```
## 3.2 Color of the wine

### 3.2.1 PCA Approach

Our first approach is to use PCA. We first construct a PCA model for the dataset. From the summaries, we can derive components that incorporate most information about the dataset. More specifically, PC1, PC2, and PC3 obtained from the dimensionality reduction algorithm can give us information about the dataset with only three variables. Next, we examine the direction of the top three principal components, namely how PCx relates to the 11 chemical properties in detail.

```{r}
pca1<-prcomp(new_wine,scale=TRUE)
summary(pca1)
plot(pca1,type="l", main = 'variances for each principal component')
loadings = pca1$rotation
```

From the summaries and plots above, we know that PC1, PC2, and PC3 collectively contain 64.36% of the information in the dataset. Thus, we take PC1, PC2, and PC3 as variables of interest for further analysis.

```{r}
a=merge(wine,pca1$x[,1:3],by="row.names")
a=rename(a,A=Row.names)
ggplot(data = a) +
  geom_point(aes(x=PC1,y=PC2, color=color), size=2) +
  labs(title = 'Scatterplot for PC1 & PC2 vs Color') +
  theme1
```

```{r}
ggplot(data = a) +
  geom_point(aes(x=PC1,y=PC3, color=color), size=2) +
  labs(title = 'Scatterplot for PC1 & PC3 vs Color') +
  theme1
```

We plot the graphs showing the relationship between PC1, PC2 & PC3, and the wines' color. The plots above demonstrate that PC1 appears to be a good indicator of the color of the wine. However, to get this information, we use the "supervised" information within the dataset, namely, the variable "color." Without variable "color," we cannot obtain the fact that PC1 relates to the color of wines (higher PC1 indicates white wine), nor can we know the cutoffs for white/red wine in PC1. Though experienced chemists might infer from the PC1 compositions (which is presented below) that PC1 is related to the color of the wine, this complicates the analysis and fails to simplify the issue for data analyst without chemistry backgrounds. Thus, PCA is not the best dimensionality reduction approach to be employed here.

```{r}
loadings[,1] %>% round(2)
```

(In PC1, citric acid, residual sugar, free sulfur dioxide, and total sulfur dioxide have positive projections while sulphates, fixed and volatile acidity have high negative projections; Experienced chemists may be able to infer from the coefficients above that PC1 is strongly correlated with colors of wines.)

### 3.2.2 Clustering Approach

Now, we attempt the clustering approach. We start by scaling the data, and pick k=2 since the wine we are studying has only two colors: red and white. We return the average chemical composition in both clusters. Details are as follows:
```{r}
X=scale(new_wine,center=TRUE,scale=TRUE)
mu=attr(X,"scaled:center")
sigma=attr(X,"scaled:scale")
#color
cluster1=kmeans(X,2,nstart=25)
```

#### Cluster1:
```{r}
(cluster1$center[1,]*sigma+mu) %>% round(2)
```

#### Cluster2:
```{r}
(cluster1$center[2,]*sigma+mu) %>% round(2)
```

From the plot demonstrating how clustering works with volatile acidity and total sulfur dioxide on the x, y-axis, and the confusion matrix, we can conclude that clustering successfully distinguishes the white wine from the red wine. 98.6% of the observations fall in the clusters of their respective colors. Cluster 1 contains the white wines predominately, whereas Cluster 2 includes mostly red wines.

```{r}
Cluster = factor(cluster1$cluster)
qplot(volatile.acidity,total.sulfur.dioxide,data=wine,color=Cluster,main="Scatterplot for Volatile Acidity & Total Sulfur Dioxide vs Color") + theme1
reg_matrix = table(wine$color,cluster1$cluster)
names(dimnames(reg_matrix)) <- list('color', 'cluster')
reg_matrix
```

In the PCA method, data analysts have to read from the technicalities of PC1 to infer that PC1 correlates with wines' color, which can be a challenge for anyone without chemistry backgrounds. In clustering, however, the dimensionality reduction technique automatically separates the observations into two groups, each one of which has predominantly one color of wines. Plus, unlike in PCA, we don't need to access the "supervised" information to determine the cutoffs for red/white wines in the clustering approach. Thus, we conclude that clustering is a better methodology than the PCA to distinguish different colors of wines with their chemical properties.

## 3.3 Quality of the wine

Next, we use clustering to distinguish between wines of different qualities. We degsinate that there are three types of wine, namely, the wines of high, average, and low qualities.

```{r}
cluster2=kmeans(X,3,nstart=25)
reg_matrix1 = table(wine$quality,cluster2$cluster)
names(dimnames(reg_matrix1)) <- list('quality', 'cluster')
reg_matrix1
Cluster = factor(cluster2$cluster)
qplot(quality,data=wine,color=Cluster,main="Histogram for Quality of Wines vs Clusters", ylab = 'Number of Observations') + theme1
```

From both the confusion matrix and the plot featuring the distribution of each cluster across different qualities of wine, we find that the clustering can't successfully distinguish between wines of different qualities. All clusters fail to show a concentration over certain levels of quality. Instead, their distributions across varying levels of quality are relatively spread out. Thus, we conclude that clustering is NOT capable of sorting the higher from the lower quality wines using their chemical compositions.

## 3.4 Conclusion
We run both PCA and Clustering algorithm on wine.csv, which contains the chemical composition of wines, and we try to find the dimensionality reduction methodology that is most effective at sorting different colors of wines. Via the analysis of 3 principal components returned from the PCA algorithm, we find it hard to sort different colors of wines without sufficient chemistry backgrounds or accessing "supervised" information. Also, though PCA maximizes the overall variance of the data along a small set of directions to concentrate information, it can well pick directions that make it hard to separate classes. Clustering, however, is proven to have distinguished the red wines from the white ones, with an accuracy of 0.986. Based on the success of the cluetring approach, we apply it to sort wines of different qualities. However, the algorithm fails to successfully sort the higher from the lower quality wines, showing that clustering is also not universal when trying to distinguish between different properties.

# 4. Market Segmentation

## 4.1 Introduction
The social_marketing.csv contains 7882 entries with 36 different categories. Each category represents a broad area of interest, like politics and sports. The entries denote the number of tweets a follower of NutrientH20 has on a given subject. The categories of interests are not mutually exclusive. This report will identify market segments that stand out among the audiences of NutrientH20 and thus provide insights into how the owner of NutrientH20 can better target its followers.

## 4.2 Clustering
Before the analysis, we first remove the variable "X" from the dataset, which is a random identification number unrelated to our study. 

We first apply the clustering methodology. After scaling the data, we use hierarchical clustering with average linkage to obtain the dendrogram below:

```{r}
social_marketing <- read.csv("/Users/pengcheng/Desktop/social_marketing.csv")
social_marketing$X <- NULL
sm_scaled = scale(social_marketing, center=TRUE, scale=TRUE)
sm_distance_matrix = dist(sm_scaled, method='euclidean')
hier_sm = hclust(sm_distance_matrix, method='average')
plot(hier_sm, cex=0.5, main = 'Hierarchical Clusters of NutrientH20 Followers', xlab='')
```

Next, we cut the tree at k = 20 to obtain 20 distinct clusters and tabulate the number of accounts in each cluster.
```{r}
library(mosaic)
library(foreach)
cluster1 = cutree(hier_sm, k=20)
summary(factor(cluster1))
num = which(cluster1 == 1)
```

From the tabulation of 20 clusters above, we find that cluster 1 contains significantly more observations than the remaining clusters. Thus, we hypothesize that cluster 1 represents the typical audience of NutrientH20 and is, therefore, the market segment that the owner of NutrientH20 should target. 

## 4.3 PCA 
```{r}
pc1 = prcomp(social_marketing, scale = TRUE)
loadings = pc1$rotation
scores = pc1$x
summary(pc1)
loadings = pc1$rotation
interests = pc1$x
```

The principal component analysis summary demonstrates that the first four principal components contain 34.08% of the information within the dataset. Next, we use the four components to predict whether an account will fall on cluster 1 using linear regression.

We generate a new variable cluster_1 denoting whether the observation belongs to cluster1. The new variable is used as the dependent variable in the linear regression, with the four principal components being the explanatory variables.

```{r}
social_marketing <- read.csv("/Users/pengcheng/Desktop/social_marketing.csv")
social_marketing$X <- NULL
social_marketing = merge(social_marketing, pc1$x[,1:4], by="row.names")
social_marketing = mutate(social_marketing, cluster_1 = 0)
social_marketing[num, 42] = 1
lm1 = glm(cluster_1~ PC1 + PC2 + PC3 + PC4, data = social_marketing)
summary(lm1)
in_cluster1 = factor(social_marketing$cluster_1)
qplot(scores[,4], scores[,1], color=in_cluster1, xlab='Component 4', ylab='Component 1', main = 'Scatterplot for PC1 & PC4 vs Cluster1') + theme1
```

The regression result shows that PC1 and PC4 are closest to being statistically significant. Whereas a higher PC1 makes it more likely to be in cluster 1, a higher PC4 makes it less likely to be cluster 1. Thus, the variables in PC1 with positive coefficients and PC4 with negative coefficients are the topics more prevalent among people in cluster 1. The details of CPC1 and PC4 is as follows:

### PC1
```{r}
loadings[,1] %>% round(2)
```

All variables in PC1 have negative coefficients. In particular, sports_fandom, religion, beauty, food, parenting, and family have the most significant absolute negative values. Namely, sports_fandom, religion, beauty, food, parenting, and family may not be welcomed by the followers of NutrientH20. Thus, the account owner should avoid posting topics of the above categories since they're unlikely to attract the largest group of the account's potential audiences.

### PC4
```{r}
loadings[,4] %>% round(2)
```

The negative coefficients within PC4 generally feature health-related topics such as personal_fitness and more serious topics like politics and news. Among the negative coefficients, health_nutrition, personal_fitness, and outdoors have the largest absolute values. This means that the typical audiences of the twitter account NutrientH20 prefer information about health_nutrition, personal_fitness, and outdoors better than others. Also, eco, travel, politics, and computers may be topics that can attract more followers to NutrientH20.

## 4.4 Conclusion
By using both clustering and PCA, we identify the cluster that is the characterization of typical followers of the account NutrientH20 and believe users in cluster1 is the market segment the owner of NutrientH20 should target. Running regression of a dummy variable denoting cluster1 and four principal components, we find that PC1 and PC4 are more statistically significant, among others. Then, from the breakdown of PC1 and PC4, we infer that NutrientH20 should focus more on ealth_nutrition, personal_fitness, and outdoors as typical users appear to be a population with an emphasis on personal health. To attract more users, the brand should accentuate its ability to share information regarding personal fitness, nutrition, and outdoor activities. Meanwhile, the brand can also share a limited amount of eco, travel, politics, and computers but avoid topics such as sports_fandom, religion, beauty, food, parenting, and family. 

In short, NutrientH20 should primarily target audiences similar to those within cluster1, which we believe represents the typical image of the account's followers by sharing information more on personal health and fitness topics. 
